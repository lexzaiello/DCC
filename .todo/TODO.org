#+TITLE: Stuff todo
#+AUTHOR: Lexi Aiello
#+DATE: 12/19/25

* Closure "combinator"

The main issue with the equational approach to dependent combinators is that, due to the lack of currying, we cannot TYPE-check partial application. How much of a limitation is this?

Example: deriving I with just {S, K} core
I = SKK, but we cannot type-check this in the current version, since it doesn't have the expected arity. We just get some A B B expression, but it gives us no meaningful information. It isn't an actual proof of \forall (\alpha : Type), \alpha \rightarrow \alpha.

Full application produces an A B B expression that beta reduces to an arrow type.

- The old approach we used in the equational type-checker was supplying a "test" / "sorry" value, then type-checking that
- In calculus of constructions, we do this by introducing a binder at the type-level. We don't have binders at the type level.

Potential approach: lazily extracting the expected types. The equational approach type-checks an application by a list of assertions:
- expected applicand types
- asserted output type

** Ideal use:

Ideally, we have a single term, which we can continuously pop from to type-check an expression.

** Approach 2a - "context" object, but with "interpretations:"

The pop design is meh right now. We want to be able to extract the assertions and the rendered type, but
as is, we might need state.

My thought is we don't actually need state, since our pop signature is:
=m \alpha \rightarrow \alpha \times m \alpha=

so we can just make a new context.
But, if we actually pop from the state, then we don't know which argument we're on.
We will need a very complicated mechanism to implement the context state.

What I'm thinking is we have multiple combinators for different steps in type-checking.
All context combinators are of type \Gamma, and they have those base methods.

So each one is stateless and carries all the info it needs to know.
We also don't need to count which argument we're on, since the combinators themselves know,
and each one has a different implementation of pop.

I wish there was a more elegant way to track arguments than using different combinators.

** Design decision: encoding - TODO

Need to decide on how we encode this in Lean.
Specifically, how we encode this in syntax.

We need to explicitly store the data as syntactical objects.
Exprs.

This gets back to the rendering issue.

Ideal use case (I \alpha x - polymorphic, not dependent):

\Gamma := push (pure \alpha) x

-- x : \alpha, just remove a pop for \alpha : Ty
\alpha = fst (pop (snd (pop (push (pure \alpha) x))
\alpha = fst (pop (snd (pop \Gamma)))

The question is the rendering. We don't actually store the rendered assertions, they're derived.

I feel like pop shouldn't actually remove an argument.

Working with the tuple is going to be mad annoying with combinators since we don't have variables.
We also don't have tuples. No pairs, so how do we do this.

I mean we will probably need pairs eventually. who cares.

pure = K context object? need some way to initialize the object.
I guess we can have multiple pure's

** What is the \Gamma combinator for?

Note: what is bind for? All call / app types are pure. we can infer their types and know it with no context. The context matters INTERNALLY to a type though.
Idea: statefully encoding interpretation rules / type rendering rules.

\Gamma allows us to for types progressively / semi-statefully instead of structurally.

For example, we would structurally encode K as:

K : \Pi (\alpha : Type) (\beta : Type), \alpha \rightarrow \beta \rightarrow \alpha

Or, we can statefully encode it.

K : Type :: (pure Type)

What problem are we trying to solve?
Meta combinator hierarchy gets extremely large.

Here's what a function app type on the user side looks like:

K x y

push 

** The flow:

1. User creates the context with \Gamma combinator methods
2. Combinator type consumes the context
3. ??

1. Creating context for K \alpha \beta x y

\Gamma := push (push (push (pure \alpha) \beta) x) y

2. "Consuming" the context

Notes:
- The combinator type has the old context \Gamma, and they are making a new context \Delta - same methods, they're talking back and forth
- Need to produce an assertion for every argument
- Want to do this in a sequential way where we can read multiple times
- We ideally can interleave the main function call and type-checking NOT uncurried

- Main difficulty with S that we tried to solve with equational is the hierarchy explosion. it becomes so gigantic
- Equational design looked promissing. We make a bunch of claims about term types, the combinator type makes a bunch of assertions,
and we proceed.
- The main challenge was we get garbage types for partially applied terms. The idea with the context object is that we can do type-checking incrementally.

-- Simpler example
I : I\Gamma

I \alpha : I\Gamma \alpha
I \alpha x : I\Gamma \alpha x

-- to check each step:
#check fst (pop (I\Gamma \alpha)) == Ty
#check fst (pop (snd (pop (I\Gamma \alpha x)))) == \alpha

uhhhhh what is the point of bind?
no point. can simplify.

Dependent K combinator

K : \forall (\alpha : Type) (\beta : \alpha \rightarrow Type) (x : \alpha) (y : \beta x), \alpha

#check read (\Gamma

This is where things might get epic?
We need to create ad-hoc \Gamma combinators.

For example, the \beta : \alpha \rightarrow Type

Can we replace arrow altogether?
This is a future context / infer rule, kinda an ad-hoc \Gamma combinator.

If we can avoid arrows altogether that would be insane, but probably unnecessary.

\alpha \rightarrow \alpha corresponds to the rule (assuming \alpha is known, not a variable):

read (next \Gamma \alpha x) = \alpha
read (next (read (next \Gamma \alpha x))) = \alpha

If we can dynamically make these rules, that would be crazy.
Then we're just passing around contexts.

The rules are characterized solely by their arity, for one thing.

Combinators are equipped with local reduction rules.

Kinda like a fixedpoint.
Weird church encoding: \Gamma combinator = create context

\Gamma : (\Gamma \rightarrow Expr (assertion)) \rightarrow \Gamma

need to concat them somehow, but the idea is, \alpha \rightarrow \alpha corresponds to:

\Gamma read (B read next)

(B read next) :: (init read) -- this is of type \Gamma

\Gamma := (B read next) :: (init read)

these are our "assertion rules:" (B read next) :: (init read)

these are our "claims" / arguments: init \alpha

Example: simply-typed I combinator

I : ((init (K Ty)) :: read :: read)

((init (K Ty)) :: read :: read) : \Gamma

I \alpha x : (((init (K Ty)) :: read :: read), \alpha :: x)

#+BEGIN_SRC
infer : Expr \rightarrow Option Expr

#+END_SRC

K : fun (c : \Gamma)

** Design decision: bind

Bind is how we do type dependency. It lets us act on the context to produce a claim

Is type dependency even relevant in \Gamma, or only in \Delta?
Take an example:

\forall (\alpha : Type), \alpha \rightarrow \alpha

What is the point of the context? It gives a way to make claims, but the claims have no context.
All of the argument type claims we make will be self contained.

** Design decision: interpretation

We have a somewhat neat way to encode our arguments, and to read them back,
but we need a plan for 

** Design decision: maintain original context?

I don't think maintaining the original context is in the scope of the \Gamma type.

** Monad approach "context"

Think through this more:

- pure denotes no self-reference to \Gamma
- bind denotes reference to previous elements of \Gamma

Analogy:
- a computation is pure / stateless
- it is stateful, but it has read only access

** Approach 1 - "context" object / monad

To note: must encode state of \Gamma algebraically / symbolically / syntactically
To note: we don't want indices, no natural numbers in the core, so it behaves like a stack? queue? some kind of list-like structure instead, maybe an adjacency list?

I wonder if we can do the assertions lazily? Produce some kind of "context" object, which we can pop / push things to...
Dependency even in CoC is forward, except in mutual induction I would guess. That is, prior arguments determine the types of future arguments.

the "context" is like a 2-channel queue? There's a channel for the inputs and the assertions, and when fully applied, there will be one more assertion than there are elements in the input queue.

This abstracts the meta combinator approach, but it doesn't necessarily condense our AIR, since we will need rewrite rules for "pop" / "read" at each step.

The context object "simulates" a context more usefully, but in a combinatory way, since it is a freestanding object, with parametricity. It behaves the same way every time.

\Gamma is the type of all contexts. MK is the context for K, for example.

class \Gamma (\alpha : Type) where
  push  : \Gamma \rightarrow \alpha \rightarrow \Gamma -- not dependent
  push' : \Gamma \rightarrow (\Gamma \rightarrow \alpha) \rightarrow \alpha \rightarrow \Gamma -- dependent on existing context

  -- \alpha is the next available assertion
  -- there will be one more assertion than there are actual arguments, since the last assertion is the rendered, fully-applied type
  pop : \Gamma \rightarrow \alpha \times \Gamma
  
-- \Gamma behaves differently, with extra constraints, between S and K for example, but how?
-- all base types in the family obey these properties, but S and K have different behavior for pop?
-- pop doesn't just push and pop types, it does so while transforming the types given the context \Gamma

-- How does this fit in?:
-- based on assertions, so it implements the equational approach
-- enables currying / partial application - can assert and read in steps

-- \Gamma is not an actual combinators, it's the push, push', and
-- pop combinators, which operate on the family of types known as \Gamma

Encoding: what kind of structure will we have, since this needs to be syntactic.
- if we pop elements out of the \Gamma, we lose some information, so we ought to render the types beforehand?
  - inside bind? there is a one-to-one mapping between pure and pop, pure a >> pop = a
- How do we handle error states? What if the user pushes too many things to the context? Just do nothing? noop.
- do we want to retain the original types at all? is this information only accessible to the type-checker?
- if we retain both, then we can express type-checking function application algebraically, which is pretty cool.

Future question: is the distinction between pure and bind at all meaningful?

To determine: is \Gamma universal? How universal? See lean code:
bind : m \alpha \rightarrow (m \alpha \rightarrow \alpha) \rightarrow m \alpha, m \alpha \rightarrow \alpha, type dependency behaves the same, the only difference is how the type dependency is implemented
the function is not the actual dependent type, it could be a function that does some work to produce it, so you can isolate that work
that is, you don't need to encode state in the context itself.

Can we encode this pattern in the pop method?
Bind is stateless, can pop also be stateless?
Worry about this later.

Another design decision: do we want m \alpha \rightarrow m \alpha, or just m \alpha \rightarrow \alpha?
- one has read-only access, but the other allows us to render the assertion in the map

Conceptual question: how does bind / a type that depends on \Gamma refer to a specific element in \Gamma? Answer: it gets the \Gamma term, which has the properties we defined, so it can pop assertions from the context, but it can't read the original types, necessarily. This is fine.

To ensure it is not stateful, we need to capture the rules for MK such that we have universal rules pertaining to all \Gamma.

Although, we can probably abstract the context logic

MK : \Gamma

push(MK
MK(\alpha, \beta, x, y) = \Gamma

read(\Gamma) = \alpha
pop(\Gamma) = 

push MS \alpha = \Gamma


* Todos 12/31/25
** TODO Should consider implementing Altenkirch's approach as a freestanding Lean thing without rewrite rules / postulates to see if his meta combinator hierarchy is more concise than ours
** Work on "equational" combinatory type theory
*** Main issue is not being able to type-check partial application. It would be amazing if we could find a way around this.
**** The difference is not in supporting or not supporting partial application, but in the mechanism: currying vs closures - add a new combinator to support this?
***** Supposedly erlang uses closures to support parital application, and every function has a fixed arity
***** Partial works similarly, using closures, but it behaves more like a macro
*** TODO Define AST, boilerplate for DSL, etc...
*** TODO 

* TODO Todos 12/27/25
** TODO Need to decide on equational vs meta combinator hierarchy
** TODO Sketch out how we would appropriate the munchausen method. seems promising, potentially.
** TODO Write more fleshed out notes on the munchausen method
** TODO Can probably do something like the CAM where we use de bruijn composition, except on type-level \lambda-abstraction to achieve dependent typing in a performant way

* Architecture III

The CAM is kind of a red-herring IMO.
It's complex and doesn't add much for what we need. We don't need full lambda calculus.
We're mainly interested in dependent SK.

Equations for SK:

There are all we need.

We want to optimize our AIR to be as flat as possible.

S x y z = x z (y z)
K x y = x

Our AIR will also depend on what datatypes we natively support.
Cairo 

Each call has a spine. 

Our spines:

k = 0
s = 1

We will need to have constraints for type-checking and evaluation, since type-checking might result in beta-reduction.

K \alpha \beta x y

This has some base type, but it is not useful.
That is, its type is M(K)(\alpha, \beta, x, y)

So type-checking is essentially just beta-reduction.

So we can unify both.

Our public input is the type, and the private input is the expression.

What are our columns?

We want to minimze our number of constraints.

Let's take a crack at an AIR for just SK.

We can have some infinite number of arguments, but 

Obviously, we want the current spine as a column.
But, we might have multiple redexes we want to perform in parallel.

For now, we can focus on just one redex.

We might have a K column, and an S column.

- K constraint:

| Step | Spine | Args |
|------+-------+------|
|      |       |      |
|      |       |      |

K x y - this is a string of integers. We could make the spine kind of the least significant one.

k = 0

012 = 1

We could do something similar to CAM, in that we use binary trees.

Something like Lafont's algebra for interaction combinators could be ideal, but even then, we don't know the size of the matrix. We need to encode our data somehow.

We need some way to flatten our data.

We can do multiplication and addition, since we're in a field.

K transition: spine goes from K to x
So we need a spine column, but how do we encode everything besides the spine?

K x y, spine = 0

There are relevant args, and then the rest of the args.

We want to do graph reduction, similarly to how Turner does it.
Turner uses BCI + SK + U + P + Y, but we only need SK, since we want a minimal ZK-AIR.

Turner implements the runtime as a graph reduction system, which is pretty standard.
I'm wondering if we can still use our interaction combinator semantics / interpretation to make evaluating parallel redexes even nicer.

Turner uses normal reduction - kinda like CBN vs CBV. CBV won't terminate as frequently as CBN will.
Graph reduction can supposedly fix some of the issues with CBN.
Normal graph reduction combines the safety advantages of CBN with the performance of CBV.
Performing redexes in parallel is a feature. Would using interaction nets make this nicer? Interaction nets are just graphical reduction under the hood. Doesn't really provide much advantage. Advantages of I Nets is that our edges are directed, so we are able to notice redexes easier--"interaction"

Structures manipulated in the system are stored in two-field cells. Notably, we need a garbage collector, whereas in interaction nets we don't.
Interaction nets also have the advantage of optimal reduction.

Some JIT optimizations: can replace I x with just x.

Combinator reduction does not have better performance, but we don't care. We want a really thin AIR.

Kind of unfair comparison, since SECD is applicative order, instead of normal order.

| Step | Spine | Relevant args | Rest args |
|------+-------+---------------+-----------|
|      |       |               |           |
|      |       |               |           |

Transition constraints must equal zero for every row.

We want to list all of our edges in the trace.
Lafont gives a nice set of equations for this.

Tall trace vs wide trace


* Architecture II

CAM has "equational" semantics. It uses pattern matching, in some sense, to give its eval rules CCC semantics.
We can give ours LCCC semantics, by:

- some new way entirely (is there a way to convert a CCC to an LCCC)
- we can use our ZK equational type theory, this intuitively makes sense

- there's nothing special about the equations given. They're just syntactic vs semantic, but we already get syntactic equations with pure SK.

* Stuff to do
** TODO May be useful to implement Lafon'ts LAM in Lean as an exercise.
** TODO How dependently-typed is Lafont's LAM?
*** TODO Lafont's LAM can support inductive types, but this would make our AIR less universal
*** If the LAM is in fact dependent, it is exactly what we will use. If it is not, we might be able to make it dependent.
** If LAM is dependent, or we have adapated it, we should implement it to get a feel
** Algebraic semantics will be critical to our ZK-AIR

* Architecture

- We're just extending the CAM with additional SK arguments for types
- This should be extendable to dependent typing in general. Dependent \lambda calculi don't have type arguments, except at the meta level

- Types are somewhat computationally relevant in dependently-typed calculi
  - So having type arguments make sense
  - Extending CAM with type arguments is a pretty minimal addition

"The first 3 rules forget an argument just as K does:"

- 0!(x,y) = x
- (n+1)!(x, y) = n!x
- ('x)y = x

- S(x, y)z = xz(yz)
- \Lambda(x)yz = x(y, z)
